{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erichschulman/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# Add warehouse data\n",
    "entry2_data = pd.read_csv(\"../data/entry_loc2.csv\")\n",
    "entry2_data = entry2_data.dropna()\n",
    "entry2_data['HD entry'] = 1*(entry2_data['HD'] > 0)\n",
    "entry2_data['LOW entry'] = 1*(entry2_data['LOW'] > 0)\n",
    "entry2_data['log income'] = np.log(1 + entry2_data['income_per_capita'])\n",
    "entry2_data['log population'] = np.log(1 + entry2_data['population'])\n",
    "entry2_data['log hd warehouse'] = np.log(1 + entry2_data['hd warehouse distance'])\n",
    "entry2_data['log low warehouse'] = np.log(1 + entry2_data['low warehouse distance'])\n",
    "entry2_data = entry2_data[(entry2_data['population']>=10000) & (entry2_data['income_per_capita']>=15000)]\n",
    "entry2_data = entry2_data[(entry2_data['lon'] <= -30) & (entry2_data['lat'] >= 25) ]\n",
    "entry2_data = sm.add_constant(entry2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def contraction(params, x, p):\n",
    "    # beta and x are kind of parameters. x is the empirical distribution of x?\n",
    "    rho = np.clip(params[-1],-1,1)\n",
    "    k = int(x.shape[1]/2)\n",
    "    util1 = np.dot(x[:, 0:k], params[0:k]) + params[-3]*p[:,1]\n",
    "    util2 = np.dot(x[:, k:2*k], params[k:2*k]) + params[-2]*p[:,0]\n",
    "    p = np.array([np.exp(util1)/(1+np.exp(util1)),\n",
    "                    np.exp(util2)/(1+np.exp(util2))])\n",
    "    \n",
    "    #summarize prob\n",
    "    prob01 = (1 - p[0, :])*(p[1, :]) \n",
    "    prob10 = (p[0, :])*(1-p[1, :]) \n",
    "    prob00 = (1-p[0, :])*(1-p[1, :]) \n",
    "    prob11 = (p[0, :])*(p[1, :]) \n",
    "    \n",
    "    #introduce the copula\n",
    "    prob01_rho = prob01*(1+rho*prob10)\n",
    "    prob10_rho = prob10*(1+rho*prob01)\n",
    "    prob00_rho = prob00*(1+rho*prob11)\n",
    "    prob11_rho = prob11*(1+rho*prob00)\n",
    "    \n",
    "    return np.array( [[prob00,prob01],[prob10,prob11]] )\n",
    "\n",
    "\n",
    "def contraction_map(betas, x, p):\n",
    "    \"\"\"final result is beliefs of firm1/firm2\"\"\"\n",
    "    full_p = None\n",
    "    for i in range(50):\n",
    "        full_p = contraction(betas, x, p)\n",
    "        \n",
    "        #use bayes rule to figure out full pr?\n",
    "        p1 = full_p[1,1] + full_p[1,0]\n",
    "        p2 = full_p[1,1] + full_p[0,1]\n",
    "        p = np.array([p1,p2]).transpose()\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.516223\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.521237\n",
      "         Iterations 6\n",
      "[-1.09364059e+01 -5.08298059e-03  9.45608270e-01  8.33899479e-03\n",
      "  4.47877722e+00 -1.15220618e+00  5.79471773e-01  5.06419108e-03\n",
      "  1.33141327e+00  1.35747936e+00  5.00000000e-01]\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "                                BayesNashLogit Results                               \n",
      "=====================================================================================\n",
      "Dep. Variable:     ['HD entry', 'LOW entry']   Log-Likelihood:                -3264.1\n",
      "Model:                        BayesNashLogit   AIC:                             6538.\n",
      "Method:                   Maximum Likelihood   BIC:                             6568.\n",
      "Date:                       Tue, 10 May 2022                                         \n",
      "Time:                               08:32:48                                         \n",
      "No. Observations:                       2952                                         \n",
      "Df Residuals:                           2947                                         \n",
      "Df Model:                                  4                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const               -11.0672        nan        nan        nan         nan         nan\n",
      "log income           -0.0117        nan        nan        nan         nan         nan\n",
      "log population        0.9867        nan        nan        nan         nan         nan\n",
      "log hd warehouse      0.0149        nan        nan        nan         nan         nan\n",
      "const                 8.1205        nan        nan        nan         nan         nan\n",
      "log income           -0.8171        nan        nan        nan         nan         nan\n",
      "log population       -0.2023        nan        nan        nan         nan         nan\n",
      "log low warehouse    -0.0097        nan        nan        nan         nan         nan\n",
      "par0                  0.7702        nan        nan        nan         nan         nan\n",
      "par1                  4.5345        nan        nan        nan         nan         nan\n",
      "par2                  0.0423        nan        nan        nan         nan         nan\n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erichschulman/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:547: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "/home/erichschulman/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "class BayesNashLogit(GenericLikelihoodModel):\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        n = self.exog.shape[0]\n",
    "        k = int(self.exog.shape[1]/2)\n",
    "        \n",
    "        p = self.endog\n",
    "        p = contraction_map(params, self.exog, p)\n",
    "        p = np.log(contraction(params, self.exog, p))\n",
    "        \n",
    "        p00 = (1 - self.endog[:, 0])*(1 - self.endog[:, 1])\n",
    "        p11 = self.endog[:, 0]*self.endog[:, 1]\n",
    "        p10 = self.endog[:, 0] * (1 - self.endog[:, 1])\n",
    "        p01 = (1 - self.endog[:, 0]) * self.endog[:, 1]\n",
    "\n",
    "        ll = p01 * p[0,1] + p10 * p[1,0] + p00 * p[0,0] + p11 * p[1,1]\n",
    "        \n",
    "        return -1*ll\n",
    "\n",
    "    \n",
    "    def fit(self, **kwds):\n",
    "        \"\"\"fit the likelihood function using the right start parameters\"\"\"\n",
    "        k = int(self.exog.shape[1]/2)\n",
    "        x1 = np.concatenate( (self.exog[:, 0:k], self.endog[:,1].reshape(self.endog.shape[0],1) ) ,axis=1)\n",
    "        x2 = np.concatenate( (self.exog[:, k:2*k], self.endog[:,0].reshape(self.endog.shape[0],1) ),axis=1)\n",
    "        params1 =  sm.Logit(self.endog[:, 0], x1).fit().params\n",
    "        params2 = sm.Logit(self.endog[:, 1], x2).fit().params\n",
    "        start_params = np.concatenate((params1[0:-1],params2[0:-1],[params1[-1],params2[-1],.5] ))\n",
    "        print(start_params)\n",
    "        return super(BayesNashLogit, self).fit(start_params=start_params, **kwds)\n",
    "    \n",
    "    \n",
    "x = entry2_data[['const','log income','log population','log hd warehouse',\n",
    "                 'const','log income','log population','log low warehouse']].copy()\n",
    "y = entry2_data[['HD entry','LOW entry']]\n",
    "\n",
    "BayesNashLogit_model = BayesNashLogit(y,x).fit()\n",
    "print(BayesNashLogit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
