{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93671413 0.08144334 0.28680139 ... 0.99824733 0.17175916 0.91664665]\n",
      " [0.50754774 0.40846412 0.99957435 ... 0.86957318 0.99959362 0.99029532]]\n"
     ]
    }
   ],
   "source": [
    "def contraction(params,x,p):\n",
    "    #beta and x are kind of parameters. x is the empirical distribution of x?\n",
    "    k = int(x.shape[1]/2)\n",
    "    util1 = np.dot(x[:,0:k], params[0:k])  + params[k]*p[0]\n",
    "    util2 = np.dot(x[:,k:], params[k+1:2*k+1]) + params[2*k+1]*p[1]\n",
    "    contr_result = [np.exp(util1)/(1+np.exp(util1)),np.exp(util2)/(1+np.exp(util2))]\n",
    "    return np.array(contr_result)\n",
    "\n",
    "#actually caclualte an equilibrium in this game\n",
    "N=1000\n",
    "ps = np.array([.5,.5])\n",
    "betas = np.array([1,-2, 2,-1])\n",
    "\n",
    "#set up xs\n",
    "xs = np.random.normal(scale=2, size=(N,2)) #assumes xs are independent? could use a copula?\n",
    "xs = pd.DataFrame(xs,columns = ['x11','x12'])\n",
    "#xs.insert(0,'x01',np.ones(N)) # I don't think the model is identified with this\n",
    "#xs.insert(2,'x02',np.ones(N))\n",
    "xs = np.array(xs)\n",
    "\n",
    "print(contraction(betas,xs,ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36869651 0.46321637]\n",
      "[0.376 0.451]\n"
     ]
    }
   ],
   "source": [
    "def contraction_map(betas,x,p):\n",
    "    \"\"\"final result is beliefs of firm1/firm2\"\"\"\n",
    "    for i in range(50):\n",
    "        #print('b1: %.4f, p2: %.4f'%tuple(p))\n",
    "        p = contraction(betas,x,p).mean(axis=1)\n",
    "        p = np.flip(p)\n",
    "        #print('p1: %.4f, p2: %.4f'%tuple(p))\n",
    "        #print('----- end of iteration %s ----'%i)\n",
    "    return p\n",
    "\n",
    "result_ps = contraction_map(betas,xs,ps)\n",
    "us = np.random.logistic(size=(1000,2))\n",
    "\n",
    "k = int(xs.shape[1]/2)\n",
    "es = np.random.logistic(0, 1, (2,N) )\n",
    "y1 = 1*(np.dot(xs[:,0:k], betas[0:k])  + betas[k]*result_ps[0] + es[0,:] >= 0)\n",
    "y2 = 1*(np.dot(xs[:,k:], betas[k+1:2*k+1]) + betas[2*k+1]*result_ps[1] + es[1,:] >= 0)\n",
    "ys = np.array([y1,y2]).transpose()\n",
    "print(np.flip(result_ps))\n",
    "print(ys.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      y1   y2       x11       x12\n",
      "0    1.0  0.0  3.694716  0.265097\n",
      "1    0.0  0.0 -1.422896  0.064841\n",
      "2    1.0  1.0  0.089030  4.130730\n",
      "3    1.0  1.0  0.260833  3.922238\n",
      "4    1.0  1.0 -1.493252  1.490628\n",
      "..   ...  ...       ...       ...\n",
      "995  1.0  0.0  2.523374  1.203708\n",
      "996  1.0  1.0  0.979598  3.193566\n",
      "997  1.0  1.0  7.344859  1.198595\n",
      "998  1.0  1.0 -0.573211  4.153911\n",
      "999  1.0  1.0  3.397633  2.562698\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df = np.concatenate( (ys,xs ) ,axis=1)\n",
    "result_df = pd.DataFrame(result_df, columns=['y1','y2','x11','x12'])\n",
    "print(result_df)\n",
    "result_df.to_csv('monte_carlo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.707240\n",
      "         Iterations: 167\n",
      "         Function evaluations: 365\n",
      "                            BayesNashLogit Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           ['y1', 'y2']   Log-Likelihood:                -707.24\n",
      "Model:                 BayesNashLogit   AIC:                             1418.\n",
      "Method:            Maximum Likelihood   BIC:                             1428.\n",
      "Date:                Thu, 01 Apr 2021                                         \n",
      "Time:                        18:15:13                                         \n",
      "No. Observations:                1000                                         \n",
      "Df Residuals:                     998                                         \n",
      "Df Model:                           1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "b11            1.0665      0.069     15.460      0.000       0.931       1.202\n",
      "d1            -2.0228      0.213     -9.479      0.000      -2.441      -1.605\n",
      "b12            1.9775      0.130     15.267      0.000       1.724       2.231\n",
      "d2            -1.3351      0.291     -4.588      0.000      -1.905      -0.765\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class BayesNashLogit(GenericLikelihoodModel):\n",
    "    \n",
    "    def nloglikeobs(self, params):\n",
    "        n = self.exog.shape[0]\n",
    "        k = int(self.exog.shape[1]/2)\n",
    "        \n",
    "        p = self.endog.mean(axis=0)\n",
    "        p = contraction_map(params,self.exog,p)\n",
    "        \n",
    "        likelihood = contraction(params,self.exog,p).transpose()\n",
    "        ll = self.endog*np.log(likelihood) + (1-self.endog)*np.log(1-likelihood)\n",
    "        \n",
    "        return -1*ll.sum()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, **kwds):\n",
    "        \"\"\"fit the likelihood function using the right start parameters\"\"\"\n",
    "        start_params = np.ones(self.exog.shape[1]+3)\n",
    "        start_params = np.array(betas)\n",
    "        return super(BayesNashLogit, self).fit(start_params=start_params,**kwds)\n",
    "\n",
    "        \n",
    "        \n",
    "N = result_df.shape[0]\n",
    "ys = result_df[['y1','y2']]\n",
    "xs = result_df[['x11' ,'x12']]\n",
    "\n",
    "\n",
    "model = BayesNashLogit(ys,xs)\n",
    "model_fit = model.fit(xtol=1e-12,ftol=1e-12)\n",
    "print(model_fit.summary(xname=[\"b11\",\"d1\",\"b12\",\"d2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06898159 0.21339743 0.12952919 0.29097551]\n"
     ]
    }
   ],
   "source": [
    "#attempt at standard errors...\n",
    "H = np.linalg.inv(model.hessian(model_fit.params)/N)\n",
    "diag = np.diagonal(np.linalg.inv(model.hessian(model_fit.params)))\n",
    "print(np.sqrt(-1*diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
